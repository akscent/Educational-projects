{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akscent/feature-extraction-classifer-txt?scriptVersionId=149040261\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"\n!pip install pymorphy2 cleantext -U nlp_profiler textblob pymystem3\nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nsys.path.insert(1, '/kaggle/input/ods-huawei/nlp_huawei_new2_task-master/nlp_huawei_new2_task-master/baseline_transformers')\n# from dataset import *\n# from model import *\n# from trainer import Trainer\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom typing import Dict\nimport json\nfrom numpy import asarray\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam, AdamW\nfrom tqdm.notebook import tqdm\nfrom textblob import TextBlob\n\ntorch.manual_seed(42)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-02T12:18:52.369806Z","iopub.execute_input":"2023-11-02T12:18:52.370334Z","iopub.status.idle":"2023-11-02T12:19:18.846214Z","shell.execute_reply.started":"2023-11-02T12:18:52.3703Z","shell.execute_reply":"2023-11-02T12:19:18.845272Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pymorphy2\n  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cleantext\n  Downloading cleantext-1.1.4-py3-none-any.whl (4.9 kB)\nCollecting nlp_profiler\n  Downloading nlp_profiler-0.0.3-py2.py3-none-any.whl (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: textblob in /opt/conda/lib/python3.10/site-packages (0.17.1)\nCollecting pymystem3\n  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\nCollecting dawg-python>=0.7.1 (from pymorphy2)\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nCollecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2) (0.6.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from cleantext) (3.2.4)\nCollecting nltk (from cleantext)\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting language-tool-python>=2.3.1 (from nlp_profiler)\n  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from nlp_profiler) (2.31.0)\nRequirement already satisfied: emoji>=0.5.4 in /opt/conda/lib/python3.10/site-packages (from nlp_profiler) (2.8.0)\nCollecting tqdm==4.46.0 (from nlp_profiler)\n  Downloading tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from nlp_profiler) (1.3.2)\nRequirement already satisfied: ipython>=7.12.0 in /opt/conda/lib/python3.10/site-packages (from nlp_profiler) (8.14.0)\nCollecting spacy<3.0.0,>=2.3.0 (from nlp_profiler)\n  Downloading spacy-2.3.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from nlp_profiler) (2.0.2)\nCollecting swifter>=1.0.3 (from nlp_profiler)\n  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: en-core-web-sm in /opt/conda/lib/python3.10/site-packages (from nlp_profiler) (3.6.0)\nCollecting textstat>=0.7.0 (from nlp_profiler)\n  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (0.18.2)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (0.1.6)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (3.0.38)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (2.15.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (5.9.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.12.0->nlp_profiler) (4.8.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->cleantext) (8.1.7)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->cleantext) (2023.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->nlp_profiler) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->nlp_profiler) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->nlp_profiler) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->nlp_profiler) (2023.7.22)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.0->nlp_profiler) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.0->nlp_profiler) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.0->nlp_profiler) (3.0.8)\nCollecting thinc<7.5.0,>=7.4.1 (from spacy<3.0.0,>=2.3.0->nlp_profiler)\n  Downloading thinc-7.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.0->nlp_profiler) (0.7.10)\nCollecting wasabi<1.1.0,>=0.4.0 (from spacy<3.0.0,>=2.3.0->nlp_profiler)\n  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\nCollecting srsly<1.1.0,>=1.0.2 (from spacy<3.0.0,>=2.3.0->nlp_profiler)\n  Downloading srsly-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (369 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m369.2/369.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7 (from spacy<3.0.0,>=2.3.0->nlp_profiler)\n  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.0->nlp_profiler) (68.0.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.0->nlp_profiler) (1.23.5)\nCollecting plac<1.2.0,>=0.9.6 (from spacy<3.0.0,>=2.3.0->nlp_profiler)\n  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\nRequirement already satisfied: psutil>=5.6.6 in /opt/conda/lib/python3.10/site-packages (from swifter>=1.0.3->nlp_profiler) (5.9.3)\nRequirement already satisfied: dask[dataframe]>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from swifter>=1.0.3->nlp_profiler) (2023.9.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->nlp_profiler) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nlp_profiler) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nlp_profiler) (2023.3)\nCollecting pyphen (from textstat>=0.7.0->nlp_profiler)\n  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of en-core-web-sm to determine which version is compatible with other requirements. This could take a while.\nCollecting nlp_profiler\n  Downloading nlp_profiler-0.0.2-py2.py3-none-any.whl (39 kB)\nRequirement already satisfied: tqdm>=4.46.0 in /opt/conda/lib/python3.10/site-packages (from nlp_profiler) (4.66.1)\nRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm->nlp_profiler) (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (1.0.4)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (0.10.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (6.3.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (3.1.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (3.3.0)\nRequirement already satisfied: cloudpickle>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp_profiler) (2.2.1)\nRequirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp_profiler) (2023.9.0)\nRequirement already satisfied: partd>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp_profiler) (1.4.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp_profiler) (6.0)\nRequirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp_profiler) (0.12.0)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp_profiler) (6.7.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.12.0->nlp_profiler) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.12.0->nlp_profiler) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.12.0->nlp_profiler) (0.2.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->nlp_profiler) (1.16.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.12.0->nlp_profiler) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.12.0->nlp_profiler) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.12.0->nlp_profiler) (0.2.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp_profiler) (3.15.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (3.0.9)\nRequirement already satisfied: locket in /opt/conda/lib/python3.10/site-packages (from partd>=1.2.0->dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp_profiler) (1.0.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (4.6.3)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (0.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm->nlp_profiler) (2.1.3)\nBuilding wheels for collected packages: swifter\n  Building wheel for swifter (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16511 sha256=b09b3a2ae24636d7df3f8f85e5a657aba0a83e6c1d1a7479a9431dd8d494cdc6\n  Stored in directory: /root/.cache/pip/wheels/e4/cf/51/0904952972ee2c7aa3709437065278dc534ec1b8d2ad41b443\nSuccessfully built swifter\nInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2, nltk, pymystem3, language-tool-python, cleantext, swifter, nlp_profiler\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cleantext-1.1.4 dawg-python-0.7.2 language-tool-python-2.7.1 nlp_profiler-0.0.2 nltk-3.8.1 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 pymystem3-0.2.0 swifter-1.4.0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7dd1dd013310>"},"metadata":{}}]},{"cell_type":"code","source":"class FiveDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_seq_len):\n        self.data = dataframe\n        self.text = dataframe['text'].tolist()\n        self.targets = None\n        if 'rate' in dataframe:\n            self.targets = dataframe['rate'].tolist()\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = ' '.join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_seq_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        if self.targets is not None:\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n            }\n\n    def __len__(self) -> int:\n        return len(self.text)\n    \n\nclass ModelForClassification(torch.nn.Module):\n\n    def __init__(self, model_path: str, config: Dict):\n        super(ModelForClassification, self).__init__()\n        self.model_name = model_path\n        self.config = config\n        self.n_classes = config['num_classes']\n        self.dropout_rate = config['dropout_rate']\n        self.bert = AutoModel.from_pretrained(self.model_name)\n        self.pre_classifier = torch.nn.Linear(312, 768)\n        self.dropout = torch.nn.Dropout(self.dropout_rate)\n        self.classifier = torch.nn.Linear(768, self.n_classes)\n        self.softmax = torch.nn.LogSoftmax(dim = 1)\n\n    def forward(self, input_ids, attention_mask,):\n        output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        hidden_state = output[0]\n        hidden_state = hidden_state[:, 0]\n        hidden_state = self.pre_classifier(hidden_state)\n        hidden_state = torch.nn.ReLU()(hidden_state)\n        hidden_state = self.dropout(hidden_state)\n        output = self.classifier(hidden_state)\n        output = self.softmax(output)\n        return output\n\n\nclass Trainer:\n    def __init__(self, config: Dict, class_weights=None):\n        self.config = config\n        self.device = config['device']\n        self.n_epochs = config['n_epochs']\n        self.optimizer = None\n        self.opt_fn = lambda model: AdamW(model.parameters(), config['lr'])\n        self.model = None\n        self.history = None\n        if class_weights is not None:\n            class_weights = class_weights.to(self.device)\n            self.loss_fn = CrossEntropyLoss(weight=class_weights)\n        else:\n            self.loss_fn = CrossEntropyLoss()\n        self.device = config['device']\n        self.verbose = config.get('verbose', True)\n        \n    def save_history(self, path: str):\n        history = {\n            'train_loss': self.history['train_loss'],\n            'val_loss': self.history['val_loss'],\n            'val_acc': self.history['val_acc']\n        }\n        val_acc = sum(self.history['val_acc']) / len(self.history['val_acc'])\n        print(\"All ACCURACY = \", val_acc)\n        with open(path, 'w') as file:\n            json.dump(history, file)\n        \n    def load_history(self, path: str):\n        with open(path, 'r') as file:\n            history = json.load(file)\n        self.history = {\n            'train_loss': history['train_loss'],\n            'val_loss': history['val_loss'],\n            'val_acc': history['val_acc']\n        }\n\n    def fit(self, model, train_dataloader, val_dataloader):\n        self.model = model.to(self.device)\n        self.optimizer = self.opt_fn(model)\n        self.history = {\n            'train_loss': [],\n            'val_loss': [],\n            'val_acc': []\n        }\n        best_val_loss = float('inf')\n\n        for epoch in range(self.n_epochs):\n            print(f\"Epoch {epoch + 1}/{self.n_epochs}\")\n            train_info = self.train_epoch(train_dataloader)\n            val_info = self.val_epoch(val_dataloader)\n            self.history['train_loss'].extend(train_info['loss'])\n            self.history['val_loss'].extend([val_info['loss']])\n            self.history['val_acc'].extend([val_info['acc']])\n\n            if val_info['loss'] < best_val_loss:\n                best_val_loss = val_info['loss']\n                self.save_model_weights('best_model_weights.ckpt')\n\n            self.save_history('history.json')\n\n        return self.model.eval()\n\n    def save_model_weights(self, path: str):\n        torch.save(self.model.state_dict(), path)\n\n\n\n    def train_epoch(self, train_dataloader):\n        self.model.train()\n        losses = []\n        total_loss = 0\n        if self.verbose:\n            train_dataloader = tqdm(train_dataloader)\n        for batch in train_dataloader:\n            ids = batch['ids'].to(self.device, dtype=torch.long)\n            mask = batch['mask'].to(self.device, dtype=torch.long)\n            targets = batch['targets'].to(self.device, dtype=torch.long)\n\n            outputs = self.model(ids, mask)\n            loss = self.loss_fn(outputs, targets)\n            total_loss += loss.item()\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            loss_val = loss.item()\n            if self.verbose:\n                train_dataloader.set_description(f\"Loss={loss_val:.3}\")\n            losses.append(loss_val)\n        avg_loss = total_loss / len(train_dataloader)\n        print(\"AVG LOSS = \", avg_loss)\n        return {'loss': losses}\n\n    def val_epoch(self, val_dataloader):\n        self.model.eval()\n        all_logits = []\n        all_labels = []\n        if self.verbose:\n            val_dataloader = tqdm(val_dataloader)\n        with torch.no_grad():\n            for batch in val_dataloader:\n                ids = batch['ids'].to(self.device, dtype=torch.long)\n                mask = batch['mask'].to(self.device, dtype=torch.long)\n                targets = batch['targets'].to(self.device, dtype=torch.long)\n                outputs = self.model(ids, mask)\n                all_logits.append(outputs)\n                all_labels.append(targets)\n        all_labels = torch.cat(all_labels).to(self.device)\n        all_logits = torch.cat(all_logits).to(self.device)\n        loss = self.loss_fn(all_logits, all_labels).item()\n        acc = (all_logits.argmax(1) == all_labels).float().mean().item()\n        print(\"ACCURACY for EPOCH = \", acc)\n        if self.verbose:\n            val_dataloader.set_description(f\"Loss={loss:.3}; Acc:{acc:.3}\")\n        return {\n            'acc': acc,\n            'loss': loss\n        }\n\n    def predict(self, test_dataloader):\n        if not self.model:\n            raise RuntimeError(\"You should train the model first\")\n        self.model.eval()\n        predictions = []\n        with torch.no_grad():\n            for batch in test_dataloader:\n                ids = batch['ids'].to(self.device, dtype=torch.long)\n                mask = batch['mask'].to(self.device, dtype=torch.long)\n                outputs = self.model(ids, mask)\n                preds = torch.exp(outputs)\n                predictions.extend(preds.tolist())\n        return asarray(predictions)\n\n    def save(self, path: str):\n        if self.model is None:\n            raise RuntimeError(\"You should train the model first\")\n        checkpoint = {\n            \"config\": self.model.config,\n            \"trainer_config\": self.config,\n            \"model_name\": self.model.model_name,\n            \"model_state_dict\": self.model.state_dict()\n        }\n        torch.save(checkpoint, path)\n\n    def plot_history(self):\n        import matplotlib.pyplot as plt\n        \n        if self.history is None:\n            raise RuntimeError(\"History is not available. Train the model first.\")\n\n        train_loss = self.history['train_loss']\n        val_loss = self.history['val_loss']\n        val_acc = self.history['val_acc']\n\n        epochs = range(1, len(train_loss) + 1)\n\n        plt.figure(figsize=(12, 5))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, train_loss, 'bo', label='Training loss')\n        plt.plot(epochs, val_loss, 'r', label='Validation loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, val_acc, 'g', label='Validation accuracy')\n        plt.title('Validation Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n\n        plt.show()\n\n\n    @classmethod\n    def load(cls, path: str):\n        ckpt = torch.load(path)\n        keys = [\"config\", \"trainer_config\", \"model_state_dict\"]\n        for key in keys:\n            if key not in ckpt:\n                raise RuntimeError(f\"Missing key {key} in checkpoint\")\n        new_model = ModelForClassification(\n            ckpt['model_name'],\n            ckpt[\"config\"]\n        )\n        new_model.load_state_dict(ckpt[\"model_state_dict\"])\n        new_trainer = cls(ckpt[\"trainer_config\"])\n        new_trainer.model = new_model\n        new_trainer.model.to(new_trainer.device)\n        return new_trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:19:18.849764Z","iopub.execute_input":"2023-11-02T12:19:18.850193Z","iopub.status.idle":"2023-11-02T12:19:18.894502Z","shell.execute_reply.started":"2023-11-02T12:19:18.850167Z","shell.execute_reply":"2023-11-02T12:19:18.893615Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob\n\n# Создание объекта TextBlob с текстом на русском языке\ntext = \"Привет, как дела?\"\nblob = TextBlob(text)\n\n\n# Извлечение основы слов\nprint(\"Основы слов:\", [word.lemmatize() for word in blob.words])\n\n# Анализ тональности\nprint(\"Тональность текста:\", blob.sentiment)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:19:18.895531Z","iopub.execute_input":"2023-11-02T12:19:18.895895Z","iopub.status.idle":"2023-11-02T12:19:20.99613Z","shell.execute_reply.started":"2023-11-02T12:19:18.895863Z","shell.execute_reply":"2023-11-02T12:19:20.995167Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Основы слов: ['Привет', 'как', 'дела']\nТональность текста: Sentiment(polarity=0.0, subjectivity=0.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data load","metadata":{}},{"cell_type":"code","source":"PATH = \"/kaggle/input/ods-huawei/\"\ntrain_data = pd.read_csv(os.path.join(PATH, \"train.csv\"))\ntest_data = pd.read_csv(os.path.join(PATH, \"test.csv\"))\nle = LabelEncoder()\ntrain_data.rate = le.fit_transform(train_data.rate)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T10:43:38.176982Z","iopub.execute_input":"2023-11-02T10:43:38.177371Z","iopub.status.idle":"2023-11-02T10:43:38.463285Z","shell.execute_reply.started":"2023-11-02T10:43:38.177343Z","shell.execute_reply":"2023-11-02T10:43:38.462364Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"   rate                                               text\n0     3  Очень понравилось. Были в начале марта  с соба...\n1     4  В целом магазин устраивает.\\nАссортимент позво...\n2     4  Очень хорошо что открылась 5 ка, теперь не над...\n3     2  Пятёрочка громко объявила о том как она заботи...\n4     2  Тесно, вечная сутолока, между рядами трудно ра...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rate</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Очень понравилось. Были в начале марта  с соба...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>В целом магазин устраивает.\\nАссортимент позво...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Очень хорошо что открылась 5 ка, теперь не над...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Пятёрочка громко объявила о том как она заботи...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Тесно, вечная сутолока, между рядами трудно ра...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pre-clean text","metadata":{}},{"cell_type":"code","source":"# import re\n# TOKEN_RE = re.compile(r'[а-яё]+')\n\n# def tokenize_text(text, min_length_token=1):\n#     text = text.lower()\n#     tokens = TOKEN_RE.findall(text)\n#     return [token for token in tokens if len(token) >= min_length_token]\n\n# def text_cleaning(text):\n#     tokens = tokenize_text(text)\n#     return ' '.join(tokens)\n\n# tqdm.pandas()\n# train_data['text'] = train_data['text'].progress_apply(text_cleaning)\n# test_data['text'] = test_data['text'].progress_apply(text_cleaning)\n\nimport re\nimport pymorphy2\n\nfrom nltk.corpus import stopwords\n\nru_stopwords = stopwords.words('russian')\ndigits = [str(i) for i in range(10)]\n\nTOKEN_RE = re.compile(r'[а-яё!.,?%]+')\nlemmatizer = pymorphy2.MorphAnalyzer()\n\ndef is_valid_word(word):\n    if not word[0].isdigit() and word not in ru_stopwords:\n        parsed_word = lemmatizer.normal_forms(word)[0]\n        return parsed_word\n    return False\n\ndef text_cleaning(text):\n    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s.,!?]', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    words = text.split()\n    cleaned_words = [word for word in words[:512] if is_valid_word(word) and len(word) < 15]\n    cleaned_text = ' '.join(cleaned_words)\n    return cleaned_text\n\ntqdm.pandas()\ntrain_data['text'] = train_data['text'].progress_apply(text_cleaning)\ntest_data['text'] = test_data['text'].progress_apply(text_cleaning)\n\ntrain_data[\"num_words\"] = train_data[\"text\"].apply(\n    lambda x: len(str(x).split()))\ntest_data[\"num_words\"] = test_data[\"text\"].apply(\n    lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:25:20.856457Z","iopub.execute_input":"2023-11-02T12:25:20.856873Z","iopub.status.idle":"2023-11-02T12:25:21.085304Z","shell.execute_reply.started":"2023-11-02T12:25:20.856845Z","shell.execute_reply":"2023-11-02T12:25:21.08449Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# train_data.to_csv(\"cleaned_train.csv\", index=False)\n# test_data.to_csv(\"cleaned_test.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:52:23.592141Z","iopub.execute_input":"2023-11-02T09:52:23.593014Z","iopub.status.idle":"2023-11-02T09:52:24.013543Z","shell.execute_reply.started":"2023-11-02T09:52:23.592981Z","shell.execute_reply":"2023-11-02T09:52:24.012516Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# del zero\ntrain_data = train_data[train_data['num_words'] != 0]\ntest_data = test_data[test_data['num_words'] != 0]","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:28:09.545003Z","iopub.execute_input":"2023-11-02T12:28:09.545388Z","iopub.status.idle":"2023-11-02T12:28:09.563748Z","shell.execute_reply.started":"2023-11-02T12:28:09.54536Z","shell.execute_reply":"2023-11-02T12:28:09.562987Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\ndef remove_infrequent_words(dataset, min_count=3):\n    word_counter = Counter()\n    for text in dataset:\n        words = text.split()\n        word_counter.update(words)\n    infrequent_words = [word for word, count in word_counter.items() if count < min_count]\n    def remove_infrequent(text):\n        words = text.split()\n        cleaned_words = [word for word in words if word not in infrequent_words]\n        cleaned_text = ' '.join(cleaned_words)\n        return cleaned_text\n    cleaned_dataset = [remove_infrequent(text) for text in tqdm(dataset, desc=\"Cleaning text\")]\n\n    return cleaned_dataset\n\ncleaned_train = remove_infrequent_words(train_data['text'].tolist())\ncleaned_test = remove_infrequent_words(test_data['text'].tolist())\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T10:47:40.039711Z","iopub.execute_input":"2023-11-02T10:47:40.039972Z","iopub.status.idle":"2023-11-02T10:58:36.064401Z","shell.execute_reply.started":"2023-11-02T10:47:40.039949Z","shell.execute_reply":"2023-11-02T10:58:36.063384Z"},"trusted":true},"execution_count":65,"outputs":[{"output_type":"display_data","data":{"text/plain":"Cleaning text:   0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23eeebe6ce7849e3a576288863a3394a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Cleaning text:   0%|          | 0/12167 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d93adbe6b7f44eb952f98c3d1f2b6ad"}},"metadata":{}}]},{"cell_type":"code","source":"train_data['cleaned_text'] = cleaned_train\ntest_data['cleaned_text'] = cleaned_test\ntrain_data.to_csv(\"cleaned_train.csv\", index=False)\ntest_data.to_csv(\"cleaned_test.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T10:58:36.066444Z","iopub.execute_input":"2023-11-02T10:58:36.066743Z","iopub.status.idle":"2023-11-02T10:58:36.783497Z","shell.execute_reply.started":"2023-11-02T10:58:36.066717Z","shell.execute_reply":"2023-11-02T10:58:36.782435Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# New data","metadata":{}},{"cell_type":"code","source":"PATH = \"/kaggle/input/cleaned-text\"\ntrain_data = pd.read_csv(os.path.join(PATH, \"cleaned_train (1).csv\"))\ntest_data = pd.read_csv(os.path.join(PATH, \"cleaned_test (1).csv\"))\n# del zero\ntrain_data = train_data[train_data['num_words'] != 0]\ntest_data = test_data[test_data['num_words'] != 0]\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:20:32.788293Z","iopub.execute_input":"2023-11-02T12:20:32.789167Z","iopub.status.idle":"2023-11-02T12:20:33.441836Z","shell.execute_reply.started":"2023-11-02T12:20:32.789137Z","shell.execute_reply":"2023-11-02T12:20:33.440896Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   rate                                               text  num_words  \\\n0     3  Очень понравилось. Были начале марта собакой. ...         29   \n1     4  В целом магазин устраивает. Ассортимент позвол...         39   \n2     4            Очень открылась ка, далеко ехать рядом!          6   \n3     2  Пятрочка громко объявила заботится пенсионерах...         26   \n4     2  Тесно, вечная сутолока, рядами трудно разойтис...         12   \n\n                                        cleaned_text  \n0  Очень понравилось. Были начале марта собакой. ...  \n1  В целом магазин устраивает. Ассортимент позвол...  \n2            Очень открылась ка, далеко ехать рядом!  \n3  Пятрочка громко заботится часы посещения магаз...  \n4  Тесно, вечная рядами трудно разойтись, грязно....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rate</th>\n      <th>text</th>\n      <th>num_words</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>29</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>39</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>6</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Пятрочка громко объявила заботится пенсионерах...</td>\n      <td>26</td>\n      <td>Пятрочка громко заботится часы посещения магаз...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Тесно, вечная сутолока, рядами трудно разойтис...</td>\n      <td>12</td>\n      <td>Тесно, вечная рядами трудно разойтись, грязно....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# replace nan\n\ndef replace_nan_with_text(row):\n    if pd.isna(row['cleaned_text']):\n        return row['text']\n    return row['cleaned_text']\n\ntrain_data['cleaned_text'] = train_data.progress_apply(replace_nan_with_text, axis=1)\ntest_data['cleaned_text'] = test_data.progress_apply(replace_nan_with_text, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:36:52.289989Z","iopub.execute_input":"2023-11-02T12:36:52.290379Z","iopub.status.idle":"2023-11-02T12:36:53.36383Z","shell.execute_reply.started":"2023-11-02T12:36:52.290353Z","shell.execute_reply":"2023-11-02T12:36:53.362988Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c13c4601da0440ba398b001db1c3653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12165 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b507ee5c20fb4defb7a8bbc9736b69bb"}},"metadata":{}}]},{"cell_type":"code","source":"def truncate_text(text, max_words=512):\n    words = text.split()\n    if len(words) > max_words:\n        truncated_text = ' '.join(words[:max_words])\n    else:\n        truncated_text = text\n    return truncated_text\n\ntqdm.pandas()\ntrain_data['cleaned_text'] = train_data['cleaned_text'].progress_apply(truncate_text)\ntest_data['cleaned_text'] = test_data['cleaned_text'].progress_apply(truncate_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:37:06.600624Z","iopub.execute_input":"2023-11-02T12:37:06.601315Z","iopub.status.idle":"2023-11-02T12:37:06.854512Z","shell.execute_reply.started":"2023-11-02T12:37:06.601282Z","shell.execute_reply":"2023-11-02T12:37:06.853634Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45bd13dda9d04726b6179466ff1b6dd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12165 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb16a76b07e44d978dd0c09514314e01"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# идея суммирования текста в более короткий текст\n\n\nfrom transformers import MBartTokenizer, MBartForConditionalGeneration\n\nmodel_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\ntokenizer = MBartTokenizer.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\ndef summary_rows(article_text):\n    input_ids = tokenizer(\n        [article_text],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )[\"input_ids\"]\n\n    output_ids = model.generate(\n        input_ids=input_ids,\n        no_repeat_ngram_size=4\n    )[0]\n\n    summary = tokenizer.decode(output_ids, skip_special_tokens=True)\n    return summary\n\ndef text_summary(text):\n    if isinstance(text, str) and text.strip() and len(str(text).split()) > 150:\n        return summary_rows(text)\n    else:\n        return text\n    \n\ntrain_data['summary'] = train_data['cleaned_text'].progress_apply(text_summary)\ntest_data['summary'] = test_data['cleaned_text'].progress_apply(text_summary)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:06:53.929167Z","iopub.execute_input":"2023-11-02T13:06:53.929998Z","iopub.status.idle":"2023-11-02T13:12:13.695153Z","shell.execute_reply.started":"2023-11-02T13:06:53.929963Z","shell.execute_reply":"2023-11-02T13:12:13.694172Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4098a36d516f4f68af7c12040fe2c9fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12165 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cacd7bf0057f41cf9ba3dcdf14bbc907"}},"metadata":{}}]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:16:56.244022Z","iopub.execute_input":"2023-11-02T13:16:56.244703Z","iopub.status.idle":"2023-11-02T13:16:56.261445Z","shell.execute_reply.started":"2023-11-02T13:16:56.24467Z","shell.execute_reply":"2023-11-02T13:16:56.260356Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"       rate                                               text  num_words  \\\n0         3  Очень понравилось. Были начале марта собакой. ...         29   \n1         4  В целом магазин устраивает. Ассортимент позвол...         39   \n2         4            Очень открылась ка, далеко ехать рядом!          6   \n3         2  Пятрочка громко объявила заботится пенсионерах...         26   \n4         2  Тесно, вечная сутолока, рядами трудно разойтис...         12   \n...     ...                                                ...        ...   \n48635     4       Удобный, маленький ещ обновили другие пятрки          6   \n48636     1  Постоянно обман цене,написанна сумма акции ито...         26   \n48637     1  Очень хочется пожелать этому магазину стать та...          9   \n48638     4  Нравится ваш магазин, персонал одекватный, пор...          6   \n48639     2                                  Не очень персонал          3   \n\n                                            cleaned_text  \\\n0      Очень понравилось. Были начале марта собакой. ...   \n1      В целом магазин устраивает. Ассортимент позвол...   \n2                Очень открылась ка, далеко ехать рядом!   \n3      Пятрочка громко заботится часы посещения магаз...   \n4      Тесно, вечная рядами трудно разойтись, грязно....   \n...                                                  ...   \n48635       Удобный, маленький ещ обновили другие пятрки   \n48636  Постоянно обман сумма акции итогу пробивают бо...   \n48637  Очень хочется пожелать этому магазину стать таким   \n48638            Нравится ваш магазин, персонал порядок.   \n48639                                  Не очень персонал   \n\n                                                 summary  \n0      Очень понравилось. Были начале марта собакой. ...  \n1      В целом магазин устраивает. Ассортимент позвол...  \n2                Очень открылась ка, далеко ехать рядом!  \n3      Пятрочка громко заботится часы посещения магаз...  \n4      Тесно, вечная рядами трудно разойтись, грязно....  \n...                                                  ...  \n48635       Удобный, маленький ещ обновили другие пятрки  \n48636  Постоянно обман сумма акции итогу пробивают бо...  \n48637  Очень хочется пожелать этому магазину стать таким  \n48638            Нравится ваш магазин, персонал порядок.  \n48639                                  Не очень персонал  \n\n[48640 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rate</th>\n      <th>text</th>\n      <th>num_words</th>\n      <th>cleaned_text</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>29</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>39</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>6</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Пятрочка громко объявила заботится пенсионерах...</td>\n      <td>26</td>\n      <td>Пятрочка громко заботится часы посещения магаз...</td>\n      <td>Пятрочка громко заботится часы посещения магаз...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Тесно, вечная сутолока, рядами трудно разойтис...</td>\n      <td>12</td>\n      <td>Тесно, вечная рядами трудно разойтись, грязно....</td>\n      <td>Тесно, вечная рядами трудно разойтись, грязно....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48635</th>\n      <td>4</td>\n      <td>Удобный, маленький ещ обновили другие пятрки</td>\n      <td>6</td>\n      <td>Удобный, маленький ещ обновили другие пятрки</td>\n      <td>Удобный, маленький ещ обновили другие пятрки</td>\n    </tr>\n    <tr>\n      <th>48636</th>\n      <td>1</td>\n      <td>Постоянно обман цене,написанна сумма акции ито...</td>\n      <td>26</td>\n      <td>Постоянно обман сумма акции итогу пробивают бо...</td>\n      <td>Постоянно обман сумма акции итогу пробивают бо...</td>\n    </tr>\n    <tr>\n      <th>48637</th>\n      <td>1</td>\n      <td>Очень хочется пожелать этому магазину стать та...</td>\n      <td>9</td>\n      <td>Очень хочется пожелать этому магазину стать таким</td>\n      <td>Очень хочется пожелать этому магазину стать таким</td>\n    </tr>\n    <tr>\n      <th>48638</th>\n      <td>4</td>\n      <td>Нравится ваш магазин, персонал одекватный, пор...</td>\n      <td>6</td>\n      <td>Нравится ваш магазин, персонал порядок.</td>\n      <td>Нравится ваш магазин, персонал порядок.</td>\n    </tr>\n    <tr>\n      <th>48639</th>\n      <td>2</td>\n      <td>Не очень персонал</td>\n      <td>3</td>\n      <td>Не очень персонал</td>\n      <td>Не очень персонал</td>\n    </tr>\n  </tbody>\n</table>\n<p>48640 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"apanc/russian-sensitive-topics\")\ntokenizer = AutoTokenizer.from_pretrained(\"apanc/russian-sensitive-topics\")\ntokenizer.padding = True\ntokenizer.truncation = True\ntokenizer.max_length = 512\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=torch.device(\"cuda:0\"))\n\ndef make_pipe(text):\n    return pipe(text, return_all_scores=True)\n\ntqdm.pandas()\ntrain_data['theme_labels'] = train_data['summary'].progress_apply(make_pipe)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:34:13.566451Z","iopub.execute_input":"2023-11-02T13:34:13.566892Z","iopub.status.idle":"2023-11-02T13:43:27.920989Z","shell.execute_reply.started":"2023-11-02T13:34:13.566858Z","shell.execute_reply":"2023-11-02T13:43:27.919894Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f39212b4f76429984a06fa4a26e47e5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_label_probs(row):\n    label_probs = [label['score'] for label in row[0]]\n    return label_probs\n\ntrain_data['label_probs'] = train_data['theme_labels'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].apply(pd.Series).add_prefix('LABEL_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['theme_labels']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:51:08.264219Z","iopub.execute_input":"2023-11-02T13:51:08.265013Z","iopub.status.idle":"2023-11-02T13:51:21.477698Z","shell.execute_reply.started":"2023-11-02T13:51:08.264981Z","shell.execute_reply":"2023-11-02T13:51:21.476723Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# feature data\ntrain_data.to_csv(\"feature_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:55:00.859369Z","iopub.execute_input":"2023-11-02T13:55:00.860058Z","iopub.status.idle":"2023-11-02T13:55:42.22507Z","shell.execute_reply.started":"2023-11-02T13:55:00.860024Z","shell.execute_reply":"2023-11-02T13:55:42.223909Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# # добавление переменных о чувствах\n# from transformers import BertTokenizer, BertForSequenceClassification\n# model_name = 'Skoltech/russian-sensitive-topics'\n# tokenizer = BertTokenizer.from_pretrained(model_name)\n# model = BertForSequenceClassification.from_pretrained(model_name);\n\n# tokenized = tokenizer.batch_encode_plus(train_data[train_data[\"num_words\"] > 80]['text'][370],\n#                                         max_length = 512,\n#                                         pad_to_max_length=True,\n#                                         truncation=True,\n#                                         return_token_type_ids=False)\n\n# tokens_ids,mask = torch.tensor(tokenized['input_ids']),torch.tensor(tokenized['attention_mask']) \n\n# with torch.no_grad():\n#     model_output = model(tokens_ids,mask)\n\n# def adjust_multilabel(y, is_pred = False):\n#     y_adjusted = []\n#     for y_c in y:\n#         y_test_curr = [0]*19\n#         index = str(int(np.argmax(y_c)))\n#         y_c = target_vaiables_id2topic_dict[index]\n#     return y_c\n\n# model_output","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:36:45.408569Z","iopub.execute_input":"2023-11-02T06:36:45.408967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# тональность текста\npipe = pipeline(model=\"seara/rubert-tiny2-russian-sentiment\", device=torch.device(\"cuda:0\"))\n\ntqdm.pandas()\ntrain_data['mood'] = train_data['summary'].progress_apply(make_pipe)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:59:16.176219Z","iopub.execute_input":"2023-11-02T13:59:16.176629Z","iopub.status.idle":"2023-11-02T14:02:10.686238Z","shell.execute_reply.started":"2023-11-02T13:59:16.176598Z","shell.execute_reply":"2023-11-02T14:02:10.685167Z"},"trusted":true},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18930db4ca464d8998dadd64b66153f3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data['label_probs'] = train_data['mood'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('MOOD_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['mood']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:03:17.818576Z","iopub.execute_input":"2023-11-02T14:03:17.819225Z","iopub.status.idle":"2023-11-02T14:03:26.971199Z","shell.execute_reply.started":"2023-11-02T14:03:17.819194Z","shell.execute_reply":"2023-11-02T14:03:26.970203Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a045b5b2c974f9b83f66de8cbbe1924"}},"metadata":{}}]},{"cell_type":"code","source":"# feature data\ntrain_data.to_csv(\"feature_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:03:34.684386Z","iopub.execute_input":"2023-11-02T14:03:34.68514Z","iopub.status.idle":"2023-11-02T14:03:34.733046Z","shell.execute_reply.started":"2023-11-02T14:03:34.685108Z","shell.execute_reply":"2023-11-02T14:03:34.732163Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"       rate                                               text  num_words  \\\n0         3  Очень понравилось. Были начале марта собакой. ...         29   \n1         4  В целом магазин устраивает. Ассортимент позвол...         39   \n2         4            Очень открылась ка, далеко ехать рядом!          6   \n3         2  Пятрочка громко объявила заботится пенсионерах...         26   \n4         2  Тесно, вечная сутолока, рядами трудно разойтис...         12   \n...     ...                                                ...        ...   \n48635     4       Удобный, маленький ещ обновили другие пятрки          6   \n48636     1  Постоянно обман цене,написанна сумма акции ито...         26   \n48637     1  Очень хочется пожелать этому магазину стать та...          9   \n48638     4  Нравится ваш магазин, персонал одекватный, пор...          6   \n48639     2                                  Не очень персонал          3   \n\n                                            cleaned_text  \\\n0      Очень понравилось. Были начале марта собакой. ...   \n1      В целом магазин устраивает. Ассортимент позвол...   \n2                Очень открылась ка, далеко ехать рядом!   \n3      Пятрочка громко заботится часы посещения магаз...   \n4      Тесно, вечная рядами трудно разойтись, грязно....   \n...                                                  ...   \n48635       Удобный, маленький ещ обновили другие пятрки   \n48636  Постоянно обман сумма акции итогу пробивают бо...   \n48637  Очень хочется пожелать этому магазину стать таким   \n48638            Нравится ваш магазин, персонал порядок.   \n48639                                  Не очень персонал   \n\n                                                 summary   LABEL_0   LABEL_1  \\\n0      Очень понравилось. Были начале марта собакой. ...  0.996648  0.000086   \n1      В целом магазин устраивает. Ассортимент позвол...  0.997240  0.000076   \n2                Очень открылась ка, далеко ехать рядом!  0.982682  0.000592   \n3      Пятрочка громко заботится часы посещения магаз...  0.391624  0.005887   \n4      Тесно, вечная рядами трудно разойтись, грязно....  0.996155  0.000161   \n...                                                  ...       ...       ...   \n48635       Удобный, маленький ещ обновили другие пятрки  0.993928  0.000115   \n48636  Постоянно обман сумма акции итогу пробивают бо...  0.005893  0.732756   \n48637  Очень хочется пожелать этому магазину стать таким  0.976297  0.000925   \n48638            Нравится ваш магазин, персонал порядок.  0.996822  0.000127   \n48639                                  Не очень персонал  0.995223  0.000181   \n\n        LABEL_2   LABEL_3   LABEL_4  ...  LABEL_386  LABEL_387  LABEL_388  \\\n0      0.000343  0.000294  0.000066  ...   0.000003   0.000003   0.000002   \n1      0.000275  0.000140  0.000051  ...   0.000003   0.000003   0.000002   \n2      0.001711  0.001077  0.000292  ...   0.000008   0.000006   0.000004   \n3      0.000527  0.003289  0.001155  ...   0.000053   0.000047   0.000041   \n4      0.000396  0.000172  0.000082  ...   0.000004   0.000003   0.000002   \n...         ...       ...       ...  ...        ...        ...        ...   \n48635  0.000334  0.000135  0.000090  ...   0.000004   0.000003   0.000003   \n48636  0.000896  0.002507  0.004022  ...   0.000358   0.000367   0.000286   \n48637  0.000675  0.000224  0.000425  ...   0.000010   0.000008   0.000007   \n48638  0.000203  0.000077  0.000039  ...   0.000003   0.000003   0.000002   \n48639  0.000282  0.000169  0.000042  ...   0.000004   0.000004   0.000002   \n\n       LABEL_389  LABEL_390  LABEL_391     LABEL_392    MOOD_0    MOOD_1  \\\n0       0.000003   0.000001   0.000001  9.850864e-07  0.010388  0.986475   \n1       0.000003   0.000001   0.000001  8.909273e-07  0.680389  0.243738   \n2       0.000007   0.000003   0.000004  3.131172e-06  0.697352  0.186863   \n3       0.000060   0.000027   0.000032  2.536447e-05  0.779933  0.021010   \n4       0.000003   0.000001   0.000001  1.052022e-06  0.545592  0.025131   \n...          ...        ...        ...           ...       ...       ...   \n48635   0.000005   0.000002   0.000002  1.358161e-06  0.157865  0.816660   \n48636   0.000251   0.000170   0.000438  2.403251e-04  0.120826  0.003329   \n48637   0.000009   0.000005   0.000005  3.461709e-06  0.212692  0.730556   \n48638   0.000003   0.000001   0.000001  9.963611e-07  0.623831  0.361995   \n48639   0.000004   0.000002   0.000002  1.553563e-06  0.779856  0.008643   \n\n         MOOD_2  \n0      0.003136  \n1      0.075873  \n2      0.115785  \n3      0.199057  \n4      0.429277  \n...         ...  \n48635  0.025475  \n48636  0.875844  \n48637  0.056752  \n48638  0.014174  \n48639  0.211501  \n\n[48640 rows x 401 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rate</th>\n      <th>text</th>\n      <th>num_words</th>\n      <th>cleaned_text</th>\n      <th>summary</th>\n      <th>LABEL_0</th>\n      <th>LABEL_1</th>\n      <th>LABEL_2</th>\n      <th>LABEL_3</th>\n      <th>LABEL_4</th>\n      <th>...</th>\n      <th>LABEL_386</th>\n      <th>LABEL_387</th>\n      <th>LABEL_388</th>\n      <th>LABEL_389</th>\n      <th>LABEL_390</th>\n      <th>LABEL_391</th>\n      <th>LABEL_392</th>\n      <th>MOOD_0</th>\n      <th>MOOD_1</th>\n      <th>MOOD_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>29</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>0.996648</td>\n      <td>0.000086</td>\n      <td>0.000343</td>\n      <td>0.000294</td>\n      <td>0.000066</td>\n      <td>...</td>\n      <td>0.000003</td>\n      <td>0.000003</td>\n      <td>0.000002</td>\n      <td>0.000003</td>\n      <td>0.000001</td>\n      <td>0.000001</td>\n      <td>9.850864e-07</td>\n      <td>0.010388</td>\n      <td>0.986475</td>\n      <td>0.003136</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>39</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>0.997240</td>\n      <td>0.000076</td>\n      <td>0.000275</td>\n      <td>0.000140</td>\n      <td>0.000051</td>\n      <td>...</td>\n      <td>0.000003</td>\n      <td>0.000003</td>\n      <td>0.000002</td>\n      <td>0.000003</td>\n      <td>0.000001</td>\n      <td>0.000001</td>\n      <td>8.909273e-07</td>\n      <td>0.680389</td>\n      <td>0.243738</td>\n      <td>0.075873</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>6</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>0.982682</td>\n      <td>0.000592</td>\n      <td>0.001711</td>\n      <td>0.001077</td>\n      <td>0.000292</td>\n      <td>...</td>\n      <td>0.000008</td>\n      <td>0.000006</td>\n      <td>0.000004</td>\n      <td>0.000007</td>\n      <td>0.000003</td>\n      <td>0.000004</td>\n      <td>3.131172e-06</td>\n      <td>0.697352</td>\n      <td>0.186863</td>\n      <td>0.115785</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Пятрочка громко объявила заботится пенсионерах...</td>\n      <td>26</td>\n      <td>Пятрочка громко заботится часы посещения магаз...</td>\n      <td>Пятрочка громко заботится часы посещения магаз...</td>\n      <td>0.391624</td>\n      <td>0.005887</td>\n      <td>0.000527</td>\n      <td>0.003289</td>\n      <td>0.001155</td>\n      <td>...</td>\n      <td>0.000053</td>\n      <td>0.000047</td>\n      <td>0.000041</td>\n      <td>0.000060</td>\n      <td>0.000027</td>\n      <td>0.000032</td>\n      <td>2.536447e-05</td>\n      <td>0.779933</td>\n      <td>0.021010</td>\n      <td>0.199057</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Тесно, вечная сутолока, рядами трудно разойтис...</td>\n      <td>12</td>\n      <td>Тесно, вечная рядами трудно разойтись, грязно....</td>\n      <td>Тесно, вечная рядами трудно разойтись, грязно....</td>\n      <td>0.996155</td>\n      <td>0.000161</td>\n      <td>0.000396</td>\n      <td>0.000172</td>\n      <td>0.000082</td>\n      <td>...</td>\n      <td>0.000004</td>\n      <td>0.000003</td>\n      <td>0.000002</td>\n      <td>0.000003</td>\n      <td>0.000001</td>\n      <td>0.000001</td>\n      <td>1.052022e-06</td>\n      <td>0.545592</td>\n      <td>0.025131</td>\n      <td>0.429277</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48635</th>\n      <td>4</td>\n      <td>Удобный, маленький ещ обновили другие пятрки</td>\n      <td>6</td>\n      <td>Удобный, маленький ещ обновили другие пятрки</td>\n      <td>Удобный, маленький ещ обновили другие пятрки</td>\n      <td>0.993928</td>\n      <td>0.000115</td>\n      <td>0.000334</td>\n      <td>0.000135</td>\n      <td>0.000090</td>\n      <td>...</td>\n      <td>0.000004</td>\n      <td>0.000003</td>\n      <td>0.000003</td>\n      <td>0.000005</td>\n      <td>0.000002</td>\n      <td>0.000002</td>\n      <td>1.358161e-06</td>\n      <td>0.157865</td>\n      <td>0.816660</td>\n      <td>0.025475</td>\n    </tr>\n    <tr>\n      <th>48636</th>\n      <td>1</td>\n      <td>Постоянно обман цене,написанна сумма акции ито...</td>\n      <td>26</td>\n      <td>Постоянно обман сумма акции итогу пробивают бо...</td>\n      <td>Постоянно обман сумма акции итогу пробивают бо...</td>\n      <td>0.005893</td>\n      <td>0.732756</td>\n      <td>0.000896</td>\n      <td>0.002507</td>\n      <td>0.004022</td>\n      <td>...</td>\n      <td>0.000358</td>\n      <td>0.000367</td>\n      <td>0.000286</td>\n      <td>0.000251</td>\n      <td>0.000170</td>\n      <td>0.000438</td>\n      <td>2.403251e-04</td>\n      <td>0.120826</td>\n      <td>0.003329</td>\n      <td>0.875844</td>\n    </tr>\n    <tr>\n      <th>48637</th>\n      <td>1</td>\n      <td>Очень хочется пожелать этому магазину стать та...</td>\n      <td>9</td>\n      <td>Очень хочется пожелать этому магазину стать таким</td>\n      <td>Очень хочется пожелать этому магазину стать таким</td>\n      <td>0.976297</td>\n      <td>0.000925</td>\n      <td>0.000675</td>\n      <td>0.000224</td>\n      <td>0.000425</td>\n      <td>...</td>\n      <td>0.000010</td>\n      <td>0.000008</td>\n      <td>0.000007</td>\n      <td>0.000009</td>\n      <td>0.000005</td>\n      <td>0.000005</td>\n      <td>3.461709e-06</td>\n      <td>0.212692</td>\n      <td>0.730556</td>\n      <td>0.056752</td>\n    </tr>\n    <tr>\n      <th>48638</th>\n      <td>4</td>\n      <td>Нравится ваш магазин, персонал одекватный, пор...</td>\n      <td>6</td>\n      <td>Нравится ваш магазин, персонал порядок.</td>\n      <td>Нравится ваш магазин, персонал порядок.</td>\n      <td>0.996822</td>\n      <td>0.000127</td>\n      <td>0.000203</td>\n      <td>0.000077</td>\n      <td>0.000039</td>\n      <td>...</td>\n      <td>0.000003</td>\n      <td>0.000003</td>\n      <td>0.000002</td>\n      <td>0.000003</td>\n      <td>0.000001</td>\n      <td>0.000001</td>\n      <td>9.963611e-07</td>\n      <td>0.623831</td>\n      <td>0.361995</td>\n      <td>0.014174</td>\n    </tr>\n    <tr>\n      <th>48639</th>\n      <td>2</td>\n      <td>Не очень персонал</td>\n      <td>3</td>\n      <td>Не очень персонал</td>\n      <td>Не очень персонал</td>\n      <td>0.995223</td>\n      <td>0.000181</td>\n      <td>0.000282</td>\n      <td>0.000169</td>\n      <td>0.000042</td>\n      <td>...</td>\n      <td>0.000004</td>\n      <td>0.000004</td>\n      <td>0.000002</td>\n      <td>0.000004</td>\n      <td>0.000002</td>\n      <td>0.000002</td>\n      <td>1.553563e-06</td>\n      <td>0.779856</td>\n      <td>0.008643</td>\n      <td>0.211501</td>\n    </tr>\n  </tbody>\n</table>\n<p>48640 rows × 401 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# токичность\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# tokenizer = BertTokenizer.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n# model = BertForSequenceClassification.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n# batch = tokenizer.encode(train_data[train_data[\"num_words\"] > 80]['text'][48421], return_tensors='pt')\n# model(batch)\n\npipe = pipeline(model=\"SkolkovoInstitute/russian_toxicity_classifier\", device=torch.device(\"cuda:0\"))\n\ntqdm.pandas()\ntrain_data['toxic'] = train_data['summary'].progress_apply(make_pipe)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:06:29.607208Z","iopub.execute_input":"2023-11-02T14:06:29.607658Z","iopub.status.idle":"2023-11-02T14:15:18.766236Z","shell.execute_reply.started":"2023-11-02T14:06:29.607627Z","shell.execute_reply":"2023-11-02T14:15:18.765249Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"637597a64a5f4a79bd2e8643af7b0a42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac8a726f303456db35e10abbb09a726"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c10fa0507c34bf0911ae187c50e30e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/1.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4962c368549d455dbba8e1fdbc68da0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a57b6a7d42945c482af116b9d1b6987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1c42db25e34e2e91279122ea797f05"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data['label_probs'] = train_data['toxic'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('TOXIC_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['toxic']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:17:22.553177Z","iopub.execute_input":"2023-11-02T14:17:22.553938Z","iopub.status.idle":"2023-11-02T14:17:31.896653Z","shell.execute_reply.started":"2023-11-02T14:17:22.553904Z","shell.execute_reply":"2023-11-02T14:17:31.895878Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83040ac9b1114a629a9d32e77344bbe2"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# эмоции\nimport torch\nfrom transformers import BertForSequenceClassification, AutoTokenizer\n\nLABELS = ['neutral', 'happiness', 'sadness', 'enthusiasm', 'fear', 'anger', 'disgust']\ntokenizer = AutoTokenizer.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection')\nmodel = BertForSequenceClassification.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection')\n\n@torch.no_grad()\ndef predict_emotion(text: str) -> str:\n    \"\"\"\n        We take the input text, tokenize it, pass it through the model, and then return the predicted label\n        :param text: The text to be classified\n        :type text: str\n        :return: The predicted emotion\n    \"\"\"\n    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n    outputs = model(**inputs)\n    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n    predicted = torch.argmax(predicted, dim=1).numpy()\n        \n    return LABELS[predicted[0]]\n\n@torch.no_grad()    \ndef predict_emotions(text: str) -> list:\n    \"\"\"\n        It takes a string of text, tokenizes it, feeds it to the model, and returns a dictionary of emotions and their\n        probabilities\n        :param text: The text you want to classify\n        :type text: str\n        :return: A dictionary of emotions and their probabilities.\n    \"\"\"\n    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n    outputs = model(**inputs)\n    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n    emotions_list = {}\n    for i in range(len(predicted.numpy()[0].tolist())):\n        emotions_list[LABELS[i]] = predicted.numpy()[0].tolist()[i]\n    return emotions_list\n\ntrain_data['toxic'] = train_data['summary'].progress_apply(predict_emotions)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:20:12.685216Z","iopub.execute_input":"2023-11-02T14:20:12.685599Z","iopub.status.idle":"2023-11-02T14:24:57.905123Z","shell.execute_reply.started":"2023-11-02T14:20:12.685571Z","shell.execute_reply":"2023-11-02T14:24:57.904146Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5114c2158ad546e8a786368d588513de"}},"metadata":{}}]},{"cell_type":"code","source":"train_data['toxic'][0]","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:31:52.240886Z","iopub.execute_input":"2023-11-02T14:31:52.241277Z","iopub.status.idle":"2023-11-02T14:31:52.248065Z","shell.execute_reply.started":"2023-11-02T14:31:52.241248Z","shell.execute_reply":"2023-11-02T14:31:52.247085Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"{'neutral': 0.0004063371161464602,\n 'happiness': 0.9980689883232117,\n 'sadness': 0.000311726878862828,\n 'enthusiasm': 0.00048426855937577784,\n 'fear': 0.0002681941259652376,\n 'anger': 0.0003283586702309549,\n 'disgust': 0.0001321935123996809}"},"metadata":{}}]},{"cell_type":"code","source":"def extract_label_probs(row):\n    label_probs = [row.get(label, 0.0) for label in LABELS]\n    return label_probs\n\ntrain_data['label_probs'] = train_data['toxic'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('EMOTION_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['toxic']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:31:54.475101Z","iopub.execute_input":"2023-11-02T14:31:54.47577Z","iopub.status.idle":"2023-11-02T14:32:03.918764Z","shell.execute_reply.started":"2023-11-02T14:31:54.475737Z","shell.execute_reply":"2023-11-02T14:32:03.917974Z"},"trusted":true},"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48640 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99084a5c0819484bad54f190f4e42e72"}},"metadata":{}}]},{"cell_type":"code","source":"# feature data\ntrain_data.to_csv(\"feature_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:33:04.238436Z","iopub.execute_input":"2023-11-02T14:33:04.239379Z","iopub.status.idle":"2023-11-02T14:33:48.126658Z","shell.execute_reply.started":"2023-11-02T14:33:04.239345Z","shell.execute_reply":"2023-11-02T14:33:48.125516Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"# the end poka","metadata":{}},{"cell_type":"code","source":"# textblob - обработка текста, генерация фич https://textblob.readthedocs.io/en/dev/quickstart.html\n# еще одна библиотека для классификации текстов https://small-text.readthedocs.io/en/latest/\n# полярность слов https://polyglot.readthedocs.io/en/latest/\n# обработка фич https://github.com/jbesomi/texthero\n# фичегенерация https://github.com/neomatrix369/nlp_profiler#Notebooks\n# классификация на других предобученных моделях, перечисленных у Алерона https://github.com/a-milenkin/Competitive_Data_Science/blob/main/notebooks/9.2.1%20-%20Text_Embeddings.ipynb\n# использовать эти ноутбуки для классификации https://github.com/e0xextazy/vkcup2022-first-stage/blob/main/inference.ipynb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"num_words\"] = train_data[\"text\"].apply(\n    lambda x: len(str(x).split()))\ntest_data[\"num_words\"] = test_data[\"text\"].apply(\n    lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2023-10-31T12:53:58.196729Z","iopub.execute_input":"2023-10-31T12:53:58.197813Z","iopub.status.idle":"2023-10-31T12:53:58.359326Z","shell.execute_reply.started":"2023-10-31T12:53:58.197778Z","shell.execute_reply":"2023-10-31T12:53:58.358553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data[\"num_words\"] > 80]['text']","metadata":{"execution":{"iopub.status.busy":"2023-10-31T13:55:36.034389Z","iopub.execute_input":"2023-10-31T13:55:36.034839Z","iopub.status.idle":"2023-10-31T13:55:36.047241Z","shell.execute_reply.started":"2023-10-31T13:55:36.034807Z","shell.execute_reply":"2023-10-31T13:55:36.046207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"markdown","source":"# CLasses","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-10-31T13:56:50.609192Z","iopub.execute_input":"2023-10-31T13:56:50.609591Z","iopub.status.idle":"2023-10-31T13:56:50.657674Z","shell.execute_reply.started":"2023-10-31T13:56:50.609557Z","shell.execute_reply":"2023-10-31T13:56:50.656658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nMAX_LEN = 50\nBATCH_SIZE = 64","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T13:56:51.500027Z","iopub.execute_input":"2023-10-31T13:56:51.500959Z","iopub.status.idle":"2023-10-31T13:56:51.507224Z","shell.execute_reply.started":"2023-10-31T13:56:51.500914Z","shell.execute_reply":"2023-10-31T13:56:51.506253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:44:22.630179Z","iopub.execute_input":"2023-10-31T14:44:22.630673Z","iopub.status.idle":"2023-10-31T14:44:22.927327Z","shell.execute_reply.started":"2023-10-31T14:44:22.630639Z","shell.execute_reply":"2023-10-31T14:44:22.926361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label encoding","metadata":{}},{"cell_type":"markdown","source":"# Cleaning","metadata":{}},{"cell_type":"code","source":"%aimport nltk.corpus.reader.bracket_parse\n%autoreload 0\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:44:37.973336Z","iopub.execute_input":"2023-10-31T14:44:37.974526Z","iopub.status.idle":"2023-10-31T14:44:37.98789Z","shell.execute_reply.started":"2023-10-31T14:44:37.974477Z","shell.execute_reply":"2023-10-31T14:44:37.987056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:44:40.19084Z","iopub.execute_input":"2023-10-31T14:44:40.191636Z","iopub.status.idle":"2023-10-31T14:45:25.52016Z","shell.execute_reply.started":"2023-10-31T14:44:40.191596Z","shell.execute_reply":"2023-10-31T14:45:25.519347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\ndef show_count_by_rate(data, rate_name = None, name = \"Data\"):\n    fig = px.histogram(data, x=\"num_words\", color=rate_name, title=f\"Number of words in {name} by rate\")\n    fig.update_layout(bargap=0.2)\n\n    fig.show()\n    \nshow_count_by_rate(train_data, rate_name = \"rate\", name = 'Train_data')\nshow_count_by_rate(test_data, name = 'Test_data')","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:45:25.521988Z","iopub.execute_input":"2023-10-31T14:45:25.522662Z","iopub.status.idle":"2023-10-31T14:45:25.671015Z","shell.execute_reply.started":"2023-10-31T14:45:25.522625Z","shell.execute_reply":"2023-10-31T14:45:25.670082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# удалим все 0 и те, чье количество меньше 30\ntrain_data = train_data[train_data['num_words'] != 0]\n# train_data = train_data[train_data['num_words'] < 30]","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:45:41.270307Z","iopub.execute_input":"2023-10-31T14:45:41.270738Z","iopub.status.idle":"2023-10-31T14:45:41.284651Z","shell.execute_reply.started":"2023-10-31T14:45:41.270707Z","shell.execute_reply":"2023-10-31T14:45:41.283548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test split","metadata":{}},{"cell_type":"code","source":"# train_split, val_split = train_test_split(train_data[train_data['rate'] != 4], test_size=0.15, random_state=42, \n#                                           shuffle = True, stratify=train_data[train_data['rate'] != 4]['rate'])\n\ntrain_split, val_split = train_test_split(train_data, test_size=0.15, random_state=42, \n                                          shuffle = True, stratify=train_data['rate'])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:45:46.213648Z","iopub.execute_input":"2023-10-31T14:45:46.214031Z","iopub.status.idle":"2023-10-31T14:45:46.246988Z","shell.execute_reply.started":"2023-10-31T14:45:46.213998Z","shell.execute_reply":"2023-10-31T14:45:46.246134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.hist(train_split['rate'], bins=10, alpha=0.5, label='Train Split')\nplt.hist(val_split['rate'], bins=10, alpha=0.5, label='Validation Split')\n\nplt.xlabel('Rate')\nplt.ylabel('Frequency')\nplt.legend()\nplt.title('Histogram of Rates for Train and Validation Splits')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:45:47.779642Z","iopub.execute_input":"2023-10-31T14:45:47.780521Z","iopub.status.idle":"2023-10-31T14:45:48.144503Z","shell.execute_reply.started":"2023-10-31T14:45:47.780487Z","shell.execute_reply":"2023-10-31T14:45:48.143492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nweights = compute_class_weight(class_weight='balanced', classes=np.unique(train_split['rate']), y=train_split['rate'])\nweight_tensor = torch.FloatTensor(weights)\nprint(weights)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:45:49.288978Z","iopub.execute_input":"2023-10-31T14:45:49.289379Z","iopub.status.idle":"2023-10-31T14:45:49.30776Z","shell.execute_reply.started":"2023-10-31T14:45:49.289345Z","shell.execute_reply":"2023-10-31T14:45:49.306727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading tokenizer from pretrained","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    \"cointegrated/rubert-tiny2\", truncation=True, do_lower_case=True)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:45:52.086533Z","iopub.execute_input":"2023-10-31T14:45:52.086987Z","iopub.status.idle":"2023-10-31T14:45:52.289182Z","shell.execute_reply.started":"2023-10-31T14:45:52.086954Z","shell.execute_reply":"2023-10-31T14:45:52.288083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating datasets and dataloaders","metadata":{}},{"cell_type":"code","source":"train_dataset = FiveDataset(train_split, tokenizer, MAX_LEN)\nval_dataset = FiveDataset(val_split, tokenizer, MAX_LEN)\ntest_dataset = FiveDataset(test_data, tokenizer, MAX_LEN)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:46:05.410495Z","iopub.execute_input":"2023-10-31T14:46:05.411487Z","iopub.status.idle":"2023-10-31T14:46:05.422502Z","shell.execute_reply.started":"2023-10-31T14:46:05.411444Z","shell.execute_reply":"2023-10-31T14:46:05.421168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 100\nBATCH_SIZE = 400","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:46:05.964538Z","iopub.execute_input":"2023-10-31T14:46:05.965524Z","iopub.status.idle":"2023-10-31T14:46:05.971833Z","shell.execute_reply.started":"2023-10-31T14:46:05.965485Z","shell.execute_reply":"2023-10-31T14:46:05.970638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_params = {\"batch_size\": BATCH_SIZE,\n                \"shuffle\": True,\n                \"num_workers\": 0\n                }\n\ntest_params = {\"batch_size\": BATCH_SIZE,\n               \"shuffle\": False,\n               \"num_workers\": 0\n               }\n\ntrain_dataloader = DataLoader(train_dataset, **train_params)\nval_dataloader = DataLoader(val_dataset, **test_params)\ntest_dataloader = DataLoader(test_dataset, **test_params)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:46:06.517407Z","iopub.execute_input":"2023-10-31T14:46:06.51825Z","iopub.status.idle":"2023-10-31T14:46:06.527596Z","shell.execute_reply.started":"2023-10-31T14:46:06.518217Z","shell.execute_reply":"2023-10-31T14:46:06.526553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading pretrained model from Huggingface","metadata":{}},{"cell_type":"code","source":"config = {\n    \"num_classes\": len(np.unique(train_split['rate'])),\n    \"dropout_rate\": 0.1\n}\nmodel = ModelForClassification(\n    \"cointegrated/rubert-tiny2\",\n    config=config\n)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:46:07.887923Z","iopub.execute_input":"2023-10-31T14:46:07.88891Z","iopub.status.idle":"2023-10-31T14:46:08.289826Z","shell.execute_reply.started":"2023-10-31T14:46:07.888874Z","shell.execute_reply":"2023-10-31T14:46:08.288981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Trainer object and fitting the model","metadata":{}},{"cell_type":"code","source":"trainer_config = {\n    \"lr\": 3e-4,\n    \"n_epochs\": 3,\n    \"weight_decay\": 1e-6,\n    \"batch_size\": BATCH_SIZE,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"seed\": 42,\n}\nt = Trainer(trainer_config, class_weights=weight_tensor)\n# ,class_weights=weight_tensor","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:46:09.90588Z","iopub.execute_input":"2023-10-31T14:46:09.906737Z","iopub.status.idle":"2023-10-31T14:46:09.920318Z","shell.execute_reply.started":"2023-10-31T14:46:09.906695Z","shell.execute_reply":"2023-10-31T14:46:09.918959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.fit(\n    model,\n    train_dataloader,\n    val_dataloader\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:46:10.763116Z","iopub.execute_input":"2023-10-31T14:46:10.764035Z","iopub.status.idle":"2023-10-31T14:48:19.04622Z","shell.execute_reply.started":"2023-10-31T14:46:10.764Z","shell.execute_reply":"2023-10-31T14:48:19.045305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"t.save(\"best_baseline_model.ckpt\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:26:44.960478Z","iopub.execute_input":"2023-10-31T14:26:44.960921Z","iopub.status.idle":"2023-10-31T14:26:45.19635Z","shell.execute_reply.started":"2023-10-31T14:26:44.960888Z","shell.execute_reply":"2023-10-31T14:26:45.195357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load pretrained Model","metadata":{}},{"cell_type":"code","source":"t = Trainer.load(\"best_baseline_model.ckpt\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:26:48.26766Z","iopub.execute_input":"2023-10-31T14:26:48.268447Z","iopub.status.idle":"2023-10-31T14:26:48.877352Z","shell.execute_reply.started":"2023-10-31T14:26:48.268402Z","shell.execute_reply":"2023-10-31T14:26:48.876474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get testset predictions\n","metadata":{}},{"cell_type":"code","source":"predictions = t.predict(test_dataloader)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:49:43.825153Z","iopub.execute_input":"2023-10-31T14:49:43.825736Z","iopub.status.idle":"2023-10-31T14:49:50.387273Z","shell.execute_reply.started":"2023-10-31T14:49:43.825696Z","shell.execute_reply":"2023-10-31T14:49:50.386167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_classes = [np.argmax(probabilities) + 1 for probabilities in predictions]","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:49:50.389275Z","iopub.execute_input":"2023-10-31T14:49:50.389691Z","iopub.status.idle":"2023-10-31T14:49:50.4471Z","shell.execute_reply.started":"2023-10-31T14:49:50.389655Z","shell.execute_reply":"2023-10-31T14:49:50.446357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission\n","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(os.path.join(PATH, \"sample_submission.csv\"))\nsample_submission[\"rate\"] = predicted_classes\n# sample_submission.rate = le.inverse_transform(sample_submission.rate)\nsample_submission.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:49:50.448179Z","iopub.execute_input":"2023-10-31T14:49:50.448485Z","iopub.status.idle":"2023-10-31T14:49:50.520997Z","shell.execute_reply.started":"2023-10-31T14:49:50.448459Z","shell.execute_reply":"2023-10-31T14:49:50.520099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:49:50.5227Z","iopub.execute_input":"2023-10-31T14:49:50.522981Z","iopub.status.idle":"2023-10-31T14:49:50.55259Z","shell.execute_reply.started":"2023-10-31T14:49:50.522956Z","shell.execute_reply":"2023-10-31T14:49:50.551593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train conf matrix","metadata":{}},{"cell_type":"code","source":"predictions_val = t.predict(val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:49:55.945166Z","iopub.execute_input":"2023-10-31T14:49:55.945602Z","iopub.status.idle":"2023-10-31T14:49:59.807246Z","shell.execute_reply.started":"2023-10-31T14:49:55.945567Z","shell.execute_reply":"2023-10-31T14:49:59.806202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_classes_val = [np.argmax(probabilities) + 1 for probabilities in predictions_val]","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:49:59.809523Z","iopub.execute_input":"2023-10-31T14:49:59.810341Z","iopub.status.idle":"2023-10-31T14:49:59.85013Z","shell.execute_reply.started":"2023-10-31T14:49:59.810301Z","shell.execute_reply":"2023-10-31T14:49:59.849428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef evaluate_classification_metrics(y_true, y_pred, model_name):\n    cm = confusion_matrix(y_true, y_pred)\n    print(f\"Classification Report for {model_name}:\\n\", classification_report(y_true, y_pred))\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n\nevaluate_classification_metrics(predicted_classes_val, le.inverse_transform(val_split['rate']), \"val dataset\")","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:49:59.851133Z","iopub.execute_input":"2023-10-31T14:49:59.851398Z","iopub.status.idle":"2023-10-31T14:50:00.182108Z","shell.execute_reply.started":"2023-10-31T14:49:59.851375Z","shell.execute_reply":"2023-10-31T14:50:00.181163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}