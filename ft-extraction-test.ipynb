{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akscent/ft-extraction-test?scriptVersionId=150292455\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pymorphy2 cleantext -U pip setuptools wheel nlp_profiler textblob pymystem3 > installer_log.txt\n!pip install spacy > installer_log.txt\nimport cudf\n!load_ext cudf.pandas \nimport os\nimport sys\nimport torch\nimport json\nimport spacy\nimport io\n# import ru_core_news_md\nimport shap\nshap.initjs()\nimport pandas as pd\nimport numpy as np\n\nfrom numpy import asarray\nfrom collections import Counter\nfrom typing import Dict\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, AutoModel, MBartTokenizer, MBartForConditionalGeneration, BertTokenizer, BertForSequenceClassification\nfrom textblob import TextBlob\nfrom nlp_profiler.core import apply_text_profiling\nfrom pymystem3 import Mystem\nfrom nltk.corpus import stopwords\nfrom catboost import CatBoostClassifier\n\n# sys.path.insert(1, '/kaggle/input/ods-huawei/nlp_huawei_new2_task-master/nlp_huawei_new2_task-master/baseline_transformers')\n# from dataset import *\n# from model import *\n# from trainer import Trainer\n\ntorch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:07:38.074528Z","iopub.execute_input":"2023-11-11T16:07:38.075506Z","iopub.status.idle":"2023-11-11T16:08:12.219319Z","shell.execute_reply.started":"2023-11-11T16:07:38.075467Z","shell.execute_reply":"2023-11-11T16:08:12.217648Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pymorphy2 cleantext -U pip setuptools wheel nlp_profiler textblob pymystem3 > installer_log.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install spacy > installer_log.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext cudf.pandas\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cudf'"],"ename":"ModuleNotFoundError","evalue":"No module named 'cudf'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FiveDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_seq_len):\n        self.data = dataframe\n        if 'summary' in dataframe:\n            self.text = dataframe['summary'].tolist()\n        else:\n            self.text = dataframe['text'].tolist()\n        self.targets = None\n        if 'rate' in dataframe:\n            self.targets = dataframe['rate'].tolist()\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = ' '.join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_seq_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        if self.targets is not None:\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n            }\n\n    def __len__(self) -> int:\n        return len(self.text)\n    \n\nclass ModelForClassification(torch.nn.Module):\n\n    def __init__(self, model_path: str, config: Dict):\n        super(ModelForClassification, self).__init__()\n        self.model_name = model_path\n        self.config = config\n        self.n_classes = config['num_classes']\n        self.dropout_rate = config['dropout_rate']\n        self.bert = AutoModel.from_pretrained(self.model_name)\n        self.pre_classifier = torch.nn.Linear(312, 768)\n        self.dropout = torch.nn.Dropout(self.dropout_rate)\n        self.classifier = torch.nn.Linear(768, self.n_classes)\n        self.softmax = torch.nn.LogSoftmax(dim = 1)\n\n    def forward(self, input_ids, attention_mask,):\n        output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        hidden_state = output[0]\n        hidden_state = hidden_state[:, 0]\n        hidden_state = self.pre_classifier(hidden_state)\n        hidden_state = torch.nn.ReLU()(hidden_state)\n        hidden_state = self.dropout(hidden_state)\n        output = self.classifier(hidden_state)\n        output = self.softmax(output)\n        return output\n\n\nclass Trainer:\n    def __init__(self, config: Dict, class_weights=None):\n        self.config = config\n        self.device = config['device']\n        self.n_epochs = config['n_epochs']\n        self.optimizer = None\n        self.opt_fn = lambda model: AdamW(model.parameters(), config['lr'])\n        self.model = None\n        self.history = None\n        if class_weights is not None:\n            class_weights = class_weights.to(self.device)\n            self.loss_fn = CrossEntropyLoss(weight=class_weights)\n        else:\n            self.loss_fn = CrossEntropyLoss()\n        self.device = config['device']\n        self.verbose = config.get('verbose', True)\n        \n    def save_history(self, path: str):\n        history = {\n            'train_loss': self.history['train_loss'],\n            'val_loss': self.history['val_loss'],\n            'val_acc': self.history['val_acc']\n        }\n        val_acc = sum(self.history['val_acc']) / len(self.history['val_acc'])\n        print(\"All ACCURACY = \", val_acc)\n        with open(path, 'w') as file:\n            json.dump(history, file)\n        \n    def load_history(self, path: str):\n        with open(path, 'r') as file:\n            history = json.load(file)\n        self.history = {\n            'train_loss': history['train_loss'],\n            'val_loss': history['val_loss'],\n            'val_acc': history['val_acc']\n        }\n\n    def fit(self, model, train_dataloader, val_dataloader):\n        self.model = model.to(self.device)\n        self.optimizer = self.opt_fn(model)\n        self.history = {\n            'train_loss': [],\n            'val_loss': [],\n            'val_acc': []\n        }\n        best_val_loss = float('inf')\n\n        for epoch in range(self.n_epochs):\n            print(f\"Epoch {epoch + 1}/{self.n_epochs}\")\n            train_info = self.train_epoch(train_dataloader)\n            val_info = self.val_epoch(val_dataloader)\n            self.history['train_loss'].extend(train_info['loss'])\n            self.history['val_loss'].extend([val_info['loss']])\n            self.history['val_acc'].extend([val_info['acc']])\n\n            if val_info['loss'] < best_val_loss:\n                best_val_loss = val_info['loss']\n                self.save_model_weights('best_model_weights.ckpt')\n\n            self.save_history('history.json')\n\n        return self.model.eval()\n\n    def save_model_weights(self, path: str):\n        torch.save(self.model.state_dict(), path)\n\n\n\n    def train_epoch(self, train_dataloader):\n        self.model.train()\n        losses = []\n        total_loss = 0\n        if self.verbose:\n            train_dataloader = tqdm(train_dataloader)\n        for batch in train_dataloader:\n            ids = batch['ids'].to(self.device, dtype=torch.long)\n            mask = batch['mask'].to(self.device, dtype=torch.long)\n            targets = batch['targets'].to(self.device, dtype=torch.long)\n\n            outputs = self.model(ids, mask)\n            loss = self.loss_fn(outputs, targets)\n            total_loss += loss.item()\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            loss_val = loss.item()\n            if self.verbose:\n                train_dataloader.set_description(f\"Loss={loss_val:.3}\")\n            losses.append(loss_val)\n        avg_loss = total_loss / len(train_dataloader)\n        print(\"AVG LOSS = \", avg_loss)\n        return {'loss': losses}\n\n    def val_epoch(self, val_dataloader):\n        self.model.eval()\n        all_logits = []\n        all_labels = []\n        if self.verbose:\n            val_dataloader = tqdm(val_dataloader)\n        with torch.no_grad():\n            for batch in val_dataloader:\n                ids = batch['ids'].to(self.device, dtype=torch.long)\n                mask = batch['mask'].to(self.device, dtype=torch.long)\n                targets = batch['targets'].to(self.device, dtype=torch.long)\n                outputs = self.model(ids, mask)\n                all_logits.append(outputs)\n                all_labels.append(targets)\n        all_labels = torch.cat(all_labels).to(self.device)\n        all_logits = torch.cat(all_logits).to(self.device)\n        loss = self.loss_fn(all_logits, all_labels).item()\n        acc = (all_logits.argmax(1) == all_labels).float().mean().item()\n        print(\"ACCURACY for EPOCH = \", acc)\n        if self.verbose:\n            val_dataloader.set_description(f\"Loss={loss:.3}; Acc:{acc:.3}\")\n        return {\n            'acc': acc,\n            'loss': loss\n        }\n\n    def predict(self, test_dataloader):\n        if not self.model:\n            raise RuntimeError(\"You should train the model first\")\n        self.model.eval()\n        predictions = []\n        with torch.no_grad():\n            for batch in test_dataloader:\n                ids = batch['ids'].to(self.device, dtype=torch.long)\n                mask = batch['mask'].to(self.device, dtype=torch.long)\n                outputs = self.model(ids, mask)\n                preds = torch.exp(outputs)\n                predictions.extend(preds.tolist())\n        return asarray(predictions)\n\n    def save(self, path: str):\n        if self.model is None:\n            raise RuntimeError(\"You should train the model first\")\n        checkpoint = {\n            \"config\": self.model.config,\n            \"trainer_config\": self.config,\n            \"model_name\": self.model.model_name,\n            \"model_state_dict\": self.model.state_dict()\n        }\n        torch.save(checkpoint, path)\n\n    def plot_history(self):\n        import matplotlib.pyplot as plt\n        \n        if self.history is None:\n            raise RuntimeError(\"History is not available. Train the model first.\")\n\n        train_loss = self.history['train_loss']\n        val_loss = self.history['val_loss']\n        val_acc = self.history['val_acc']\n\n        epochs = range(1, len(train_loss) + 1)\n\n        plt.figure(figsize=(12, 5))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, train_loss, 'bo', label='Training loss')\n        plt.plot(epochs, val_loss, 'r', label='Validation loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, val_acc, 'g', label='Validation accuracy')\n        plt.title('Validation Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n\n        plt.show()\n\n\n    @classmethod\n    def load(cls, path: str):\n        ckpt = torch.load(path)\n        keys = [\"config\", \"trainer_config\", \"model_state_dict\"]\n        for key in keys:\n            if key not in ckpt:\n                raise RuntimeError(f\"Missing key {key} in checkpoint\")\n        new_model = ModelForClassification(\n            ckpt['model_name'],\n            ckpt[\"config\"]\n        )\n        new_model.load_state_dict(ckpt[\"model_state_dict\"])\n        new_trainer = cls(ckpt[\"trainer_config\"])\n        new_trainer.model = new_model\n        new_trainer.model.to(new_trainer.device)\n        return new_trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:22:22.07514Z","iopub.status.idle":"2023-11-07T09:22:22.075658Z","shell.execute_reply.started":"2023-11-07T09:22:22.075422Z","shell.execute_reply":"2023-11-07T09:22:22.075446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/ods-huawei/test.csv')\ntest_data\n# test_dataset = FiveDataset(test_data, tokenizer, MAX_LEN)\n# test_dataloader = DataLoader(test_dataset, **test_params)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:40:01.44235Z","iopub.execute_input":"2023-11-07T09:40:01.442769Z","iopub.status.idle":"2023-11-07T09:40:01.510288Z","shell.execute_reply.started":"2023-11-07T09:40:01.442741Z","shell.execute_reply":"2023-11-07T09:40:01.509248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pre-cleaning\nimport re\nimport pymorphy2\nru_stopwords = stopwords.words('russian')\ndigits = [str(i) for i in range(10)]\n\nTOKEN_RE = re.compile(r'[а-яё!.,?%]+')\nlemmatizer = pymorphy2.MorphAnalyzer()\n\ndef is_valid_word(word):\n    if not word[0].isdigit() and word not in ru_stopwords:\n        parsed_word = lemmatizer.normal_forms(word)[0]\n        return parsed_word\n    return False\n\ndef text_cleaning(text):\n    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s.,!?]', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    words = text.split()\n    cleaned_words = [word for word in words[:512] if is_valid_word(word) and len(word) < 15]\n    cleaned_text = ' '.join(cleaned_words)\n    return cleaned_text\n\ntqdm.pandas()\ntest_data['text'] = test_data['text'].progress_apply(text_cleaning)\n\ntest_data[\"num_words\"] = test_data[\"text\"].apply(\n    lambda x: len(str(x).split()))\ntest_data.loc[test_data[\"num_words\"] == 0, \"text\"] = \"нормально\"","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:40:44.002175Z","iopub.execute_input":"2023-11-07T09:40:44.003047Z","iopub.status.idle":"2023-11-07T09:41:31.678865Z","shell.execute_reply.started":"2023-11-07T09:40:44.003007Z","shell.execute_reply":"2023-11-07T09:41:31.677978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_infrequent_words(dataset, min_count=3):\n    word_counter = Counter()\n    for text in dataset:\n        words = text.split()\n        word_counter.update(words)\n    infrequent_words = [word for word, count in word_counter.items() if count < min_count]\n    def remove_infrequent(text):\n        words = text.split()\n        cleaned_words = [word for word in words if word not in infrequent_words]\n        cleaned_text = ' '.join(cleaned_words)\n        return cleaned_text\n    cleaned_dataset = [remove_infrequent(text) for text in tqdm(dataset, desc=\"Cleaning text\")]\n\n    return cleaned_dataset\n\ncleaned_test = remove_infrequent_words(test_data['text'].tolist())","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:41:31.680654Z","iopub.execute_input":"2023-11-07T09:41:31.680945Z","iopub.status.idle":"2023-11-07T09:42:30.288048Z","shell.execute_reply.started":"2023-11-07T09:41:31.68092Z","shell.execute_reply":"2023-11-07T09:42:30.286975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['cleaned_text'] = cleaned_test","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:42:30.289467Z","iopub.execute_input":"2023-11-07T09:42:30.289886Z","iopub.status.idle":"2023-11-07T09:42:30.297078Z","shell.execute_reply.started":"2023-11-07T09:42:30.289831Z","shell.execute_reply":"2023-11-07T09:42:30.296146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_nan_with_text(row):\n    if pd.isna(row['cleaned_text']):\n        return row['text']\n    return row['cleaned_text']\n\ntest_data['cleaned_text'] = test_data.progress_apply(replace_nan_with_text, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:42:30.299357Z","iopub.execute_input":"2023-11-07T09:42:30.299638Z","iopub.status.idle":"2023-11-07T09:42:30.540951Z","shell.execute_reply.started":"2023-11-07T09:42:30.299614Z","shell.execute_reply":"2023-11-07T09:42:30.539968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncate_text(text, max_words=512):\n    words = text.split()\n    if len(words) > max_words:\n        truncated_text = ' '.join(words[:max_words])\n    else:\n        truncated_text = text\n    return truncated_text\n\ntest_data['cleaned_text'] = test_data['cleaned_text'].progress_apply(truncate_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:42:30.542195Z","iopub.execute_input":"2023-11-07T09:42:30.54251Z","iopub.status.idle":"2023-11-07T09:42:30.61009Z","shell.execute_reply.started":"2023-11-07T09:42:30.542483Z","shell.execute_reply":"2023-11-07T09:42:30.609204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\ntokenizer = MBartTokenizer.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\ndef summary_rows(article_text):\n    input_ids = tokenizer(\n        [article_text],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )[\"input_ids\"]\n\n    output_ids = model.generate(\n        input_ids=input_ids,\n        no_repeat_ngram_size=4\n    )[0]\n\n    summary = tokenizer.decode(output_ids, skip_special_tokens=True)\n    return summary\n\ndef text_summary(text):\n    if isinstance(text, str) and text.strip() and len(str(text).split()) > 150:\n        return summary_rows(text)\n    else:\n        return text\n    \n\ntest_data['summary'] = test_data['cleaned_text'].progress_apply(text_summary)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:42:30.611249Z","iopub.execute_input":"2023-11-07T09:42:30.611519Z","iopub.status.idle":"2023-11-07T09:43:13.625444Z","shell.execute_reply.started":"2023-11-07T09:42:30.611494Z","shell.execute_reply":"2023-11-07T09:43:13.624511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:43:13.626885Z","iopub.execute_input":"2023-11-07T09:43:13.627276Z","iopub.status.idle":"2023-11-07T09:43:13.643683Z","shell.execute_reply.started":"2023-11-07T09:43:13.627238Z","shell.execute_reply":"2023-11-07T09:43:13.642713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.to_csv(\"test_cleaned.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:43:13.644905Z","iopub.execute_input":"2023-11-07T09:43:13.645256Z","iopub.status.idle":"2023-11-07T09:43:13.831647Z","shell.execute_reply.started":"2023-11-07T09:43:13.645224Z","shell.execute_reply":"2023-11-07T09:43:13.830933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Test Data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/ods-huawei/test_cleaned.csv\")\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:08:19.291601Z","iopub.execute_input":"2023-11-11T16:08:19.292033Z","iopub.status.idle":"2023-11-11T16:08:19.52897Z","shell.execute_reply.started":"2023-11-11T16:08:19.292Z","shell.execute_reply":"2023-11-11T16:08:19.52782Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       index                                               text  num_words  \\\n0          0          Очень хороший магазин сотрудники приятный          5   \n1          1  Самый обычный продуктовый магазин. Есть сыры, ...          9   \n2          2                                       Вс комфортно          2   \n3          3  Маленький филиальчик, необходимое есть. Две ка...          9   \n4          4                      Плохо относятся клиентам!!!!!          3   \n...      ...                                                ...        ...   \n12162  12162            Персонал вежливый . Большой ассортимент          5   \n12163  12163  Скидки сыры. Скидки алкоголь. Приемлимые цены ...         21   \n12164  12164  Рядом домом, неплохая пятерочка, персонал хоро...         12   \n12165  12165  Хороший магазин дома. Кассиры приветливые. Про...          7   \n12166  12166  Норм, побольше сотрудников доставку выходным, ...          9   \n\n                                            cleaned_text  \\\n0              Очень хороший магазин сотрудники приятный   \n1      Самый обычный продуктовый магазин. Есть сыры, ...   \n2                                           Вс комфортно   \n3      Маленький необходимое есть. Две кассы, народу ...   \n4                                        Плохо относятся   \n...                                                  ...   \n12162            Персонал вежливый . Большой ассортимент   \n12163  Скидки сыры. Скидки алкоголь. цены фрукты . Ос...   \n12164  Рядом домом, неплохая пятерочка, персонал хоро...   \n12165  Хороший магазин дома. Кассиры приветливые. Про...   \n12166  Норм, побольше сотрудников доставку т.к. доставка   \n\n                                                 summary  \n0              Очень хороший магазин сотрудники приятный  \n1      Самый обычный продуктовый магазин. Есть сыры, ...  \n2                                           Вс комфортно  \n3      Маленький необходимое есть. Две кассы, народу ...  \n4                                        Плохо относятся  \n...                                                  ...  \n12162            Персонал вежливый . Большой ассортимент  \n12163  Скидки сыры. Скидки алкоголь. цены фрукты . Ос...  \n12164  Рядом домом, неплохая пятерочка, персонал хоро...  \n12165  Хороший магазин дома. Кассиры приветливые. Про...  \n12166  Норм, побольше сотрудников доставку т.к. доставка  \n\n[12167 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>text</th>\n      <th>num_words</th>\n      <th>cleaned_text</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Очень хороший магазин сотрудники приятный</td>\n      <td>5</td>\n      <td>Очень хороший магазин сотрудники приятный</td>\n      <td>Очень хороший магазин сотрудники приятный</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Самый обычный продуктовый магазин. Есть сыры, ...</td>\n      <td>9</td>\n      <td>Самый обычный продуктовый магазин. Есть сыры, ...</td>\n      <td>Самый обычный продуктовый магазин. Есть сыры, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Вс комфортно</td>\n      <td>2</td>\n      <td>Вс комфортно</td>\n      <td>Вс комфортно</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Маленький филиальчик, необходимое есть. Две ка...</td>\n      <td>9</td>\n      <td>Маленький необходимое есть. Две кассы, народу ...</td>\n      <td>Маленький необходимое есть. Две кассы, народу ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Плохо относятся клиентам!!!!!</td>\n      <td>3</td>\n      <td>Плохо относятся</td>\n      <td>Плохо относятся</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12162</th>\n      <td>12162</td>\n      <td>Персонал вежливый . Большой ассортимент</td>\n      <td>5</td>\n      <td>Персонал вежливый . Большой ассортимент</td>\n      <td>Персонал вежливый . Большой ассортимент</td>\n    </tr>\n    <tr>\n      <th>12163</th>\n      <td>12163</td>\n      <td>Скидки сыры. Скидки алкоголь. Приемлимые цены ...</td>\n      <td>21</td>\n      <td>Скидки сыры. Скидки алкоголь. цены фрукты . Ос...</td>\n      <td>Скидки сыры. Скидки алкоголь. цены фрукты . Ос...</td>\n    </tr>\n    <tr>\n      <th>12164</th>\n      <td>12164</td>\n      <td>Рядом домом, неплохая пятерочка, персонал хоро...</td>\n      <td>12</td>\n      <td>Рядом домом, неплохая пятерочка, персонал хоро...</td>\n      <td>Рядом домом, неплохая пятерочка, персонал хоро...</td>\n    </tr>\n    <tr>\n      <th>12165</th>\n      <td>12165</td>\n      <td>Хороший магазин дома. Кассиры приветливые. Про...</td>\n      <td>7</td>\n      <td>Хороший магазин дома. Кассиры приветливые. Про...</td>\n      <td>Хороший магазин дома. Кассиры приветливые. Про...</td>\n    </tr>\n    <tr>\n      <th>12166</th>\n      <td>12166</td>\n      <td>Норм, побольше сотрудников доставку выходным, ...</td>\n      <td>9</td>\n      <td>Норм, побольше сотрудников доставку т.к. доставка</td>\n      <td>Норм, побольше сотрудников доставку т.к. доставка</td>\n    </tr>\n  </tbody>\n</table>\n<p>12167 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def replace_nan_with_text(row):\n    if pd.isna(row['summary']) :\n        return 'text'\n    return row['summary']\n\ntqdm.pandas()\ntrain_data['summary'] = train_data.progress_apply(replace_nan_with_text, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:08:43.527421Z","iopub.execute_input":"2023-11-11T16:08:43.528338Z","iopub.status.idle":"2023-11-11T16:08:43.824762Z","shell.execute_reply.started":"2023-11-11T16:08:43.528295Z","shell.execute_reply":"2023-11-11T16:08:43.823373Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12167 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59f402b086e74559b8541094aed6b775"}},"metadata":{}}]},{"cell_type":"code","source":"# тональность текста\nfrom tqdm import tqdm\n\ndef make_pipe(text):\n    return pipe(text, return_all_scores=True)\n\ndef extract_label_probs(row):\n    label_probs = [label['score'] for label in row[0]]\n    return label_probs\n\npipe = pipeline(model=\"seara/rubert-tiny2-russian-sentiment\", device=torch.device(\"cuda:0\"))\n\ntqdm.pandas()\ntrain_data['mood'] = train_data['summary'].progress_apply(make_pipe)\n\ntrain_data['label_probs'] = train_data['mood'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('MOOD_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['mood']","metadata":{"execution":{"iopub.status.busy":"2023-11-11T11:06:19.835467Z","iopub.execute_input":"2023-11-11T11:06:19.83634Z","iopub.status.idle":"2023-11-11T11:07:04.04846Z","shell.execute_reply.started":"2023-11-11T11:06:19.836306Z","shell.execute_reply":"2023-11-11T11:07:04.047337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# токичность\n\n# tokenizer = BertTokenizer.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n# model = BertForSequenceClassification.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n# batch = tokenizer.encode(train_data[train_data[\"num_words\"] > 80]['text'][48421], return_tensors='pt')\n# model(batch)\n\npipe = pipeline(model=\"SkolkovoInstitute/russian_toxicity_classifier\", device=torch.device(\"cuda:0\"))\n\ntqdm.pandas()\ntrain_data['toxic'] = train_data['summary'].progress_apply(make_pipe)\ntrain_data['label_probs'] = train_data['toxic'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('TOXIC_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['toxic']","metadata":{"execution":{"iopub.status.busy":"2023-11-11T11:07:11.727039Z","iopub.execute_input":"2023-11-11T11:07:11.727426Z","iopub.status.idle":"2023-11-11T11:09:18.571598Z","shell.execute_reply.started":"2023-11-11T11:07:11.727393Z","shell.execute_reply":"2023-11-11T11:09:18.570587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n        'HIGH_LEVEL_OPTION': False,\n        'GRANULAR_OPTION': True,\n        'GRAMMAR_CHECK_OPTION': False,  # default: False as slow process but can Enabled\n        'SPELLING_CHECK_OPTION': False,  # default: True although slightly slow process but can Disabled\n        'EASE_OF_READING_CHECK_OPTION': False,\n        'PARALLELISATION_METHOD_OPTION': 'DEFAULT_PARALLEL_METHOD',\n    }\n\nprofiled_text_dataframe = apply_text_profiling(train_data, 'text')","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:23:05.815711Z","iopub.execute_input":"2023-11-11T16:23:05.816133Z","iopub.status.idle":"2023-11-11T17:28:02.532436Z","shell.execute_reply.started":"2023-11-11T16:23:05.816103Z","shell.execute_reply":"2023-11-11T17:28:02.531108Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"final params: {'high_level': True, 'granular': True, 'grammar_check': False, 'spelling_check': True, 'parallelisation_method': 'default'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed963532906746de8c32124a65a42e74"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca1be48ae30d42acb5df90c9554e7969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb4e91119194b94a5c9b9705162bfae"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a22090fef94fb1ad655be6f386f738"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71a03510136e457a952c48dfd5bc35ea"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf8f233daec4997881c1c0beac91f7e"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f3cfa88666b4e0ab28d2ffb7f3cbfad"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b59c27231c774f26b9c31094cd24423a"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da48c50689814cf68482eb2114768cbf"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79fc44450c3a4309a764c7ccab8e858f"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18475829161f464b9efdcee51a7e6e70"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c71e20a37f674138b2404f2b25a0a07f"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b231d407c0a945e7bb05e5698365a72e"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e331bd25b54bc1b76657c83384de5c"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cda2a8bcdb644277b7d1f4504d932e9c"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d3265dfcb1e48f1a7d44097f29b682c"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24328f35d48243b59cd6cd65c73b24f6"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be2c18d8ec7492e807040cfd5c54777"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be7e27da7cb0457aaf8b1c178bde0fc6"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b4a5e7cde614b199bb1ae00b3146c76"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfe374d51b75481a8c27f12b8d8b5be3"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080e250812734d34afa114137742606d"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40980e134f62453fb0c56a329c015fff"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e8d79480b04ee4944d3490def77453"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b8f9c30c17437ba99deb88b311ac97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cbb282f6f9949e99a5bf7fd2ef191fc"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66433f3e84ff46f99cd9fa9836600e2d"}},"metadata":{}},{"name":"stderr","text":"Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n","output_type":"stream"}]},{"cell_type":"code","source":"profiled_text_dataframe","metadata":{"execution":{"iopub.status.busy":"2023-11-11T17:46:36.712564Z","iopub.execute_input":"2023-11-11T17:46:36.713118Z","iopub.status.idle":"2023-11-11T17:46:36.780018Z","shell.execute_reply.started":"2023-11-11T17:46:36.713082Z","shell.execute_reply":"2023-11-11T17:46:36.77885Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                    text  sentences_count  \\\n0              Очень хороший магазин сотрудники приятный                1   \n1      Самый обычный продуктовый магазин. Есть сыры, ...                2   \n2                                           Вс комфортно                1   \n3      Маленький филиальчик, необходимое есть. Две ка...                2   \n4                          Плохо относятся клиентам!!!!!                2   \n...                                                  ...              ...   \n12162            Персонал вежливый . Большой ассортимент                2   \n12163  Скидки сыры. Скидки алкоголь. Приемлимые цены ...                7   \n12164  Рядом домом, неплохая пятерочка, персонал хоро...                2   \n12165  Хороший магазин дома. Кассиры приветливые. Про...                3   \n12166  Норм, побольше сотрудников доставку выходным, ...                2   \n\n       characters_count  spaces_count  count_words  duplicates_count  \\\n0                    41             4            5                 0   \n1                    67             8            9                 1   \n2                    12             1            2                 0   \n3                    74             8            9                 2   \n4                    29             2            3                 1   \n...                 ...           ...          ...               ...   \n12162                39             4            4                 0   \n12163               146            20           20                 2   \n12164                98            11           12                 1   \n12165                58             6            7                 1   \n12166                76             8           10                 2   \n\n       chars_excl_spaces_count  emoji_count  whole_numbers_count  \\\n0                           37            0                    0   \n1                           59            0                    0   \n2                           11            0                    0   \n3                           66            0                    0   \n4                           27            0                    0   \n...                        ...          ...                  ...   \n12162                       35            0                    0   \n12163                      126            0                    0   \n12164                       87            0                    0   \n12165                       52            0                    0   \n12166                       68            0                    0   \n\n       alpha_numeric_count  ...  noun_phase_count  sentiment_polarity_score  \\\n0                        0  ...                 5                       0.0   \n1                        0  ...                10                       0.0   \n2                        0  ...                 2                       0.0   \n3                        0  ...                 9                       0.0   \n4                        0  ...                 3                       0.0   \n...                    ...  ...               ...                       ...   \n12162                    0  ...                 4                       0.0   \n12163                    0  ...                18                       0.0   \n12164                    0  ...                12                       0.0   \n12165                    0  ...                 6                       0.0   \n12166                    0  ...                 8                       0.0   \n\n       sentiment_polarity  sentiment_polarity_summarised  \\\n0                 Neutral                        Neutral   \n1                 Neutral                        Neutral   \n2                 Neutral                        Neutral   \n3                 Neutral                        Neutral   \n4                 Neutral                        Neutral   \n...                   ...                            ...   \n12162             Neutral                        Neutral   \n12163             Neutral                        Neutral   \n12164             Neutral                        Neutral   \n12165             Neutral                        Neutral   \n12166             Neutral                        Neutral   \n\n       sentiment_subjectivity_score  sentiment_subjectivity  \\\n0                               0.0          Very objective   \n1                               0.0          Very objective   \n2                               0.0          Very objective   \n3                               0.0          Very objective   \n4                               0.0          Very objective   \n...                             ...                     ...   \n12162                           0.0          Very objective   \n12163                           0.0          Very objective   \n12164                           0.0          Very objective   \n12165                           0.0          Very objective   \n12166                           0.0          Very objective   \n\n      sentiment_subjectivity_summarised spelling_quality_score  \\\n0                             Objective               0.000000   \n1                             Objective               0.230769   \n2                             Objective               0.000000   \n3                             Objective               0.357143   \n4                             Objective               0.625000   \n...                                 ...                    ...   \n12162                         Objective               0.200000   \n12163                         Objective               0.296296   \n12164                         Objective               0.333333   \n12165                         Objective               0.222222   \n12166                         Objective               0.307692   \n\n       spelling_quality spelling_quality_summarised  \n0              Very bad                         Bad  \n1            Pretty bad                         Bad  \n2              Very bad                         Bad  \n3            Pretty bad                         Bad  \n4                   Bad                         Bad  \n...                 ...                         ...  \n12162        Pretty bad                         Bad  \n12163        Pretty bad                         Bad  \n12164        Pretty bad                         Bad  \n12165        Pretty bad                         Bad  \n12166        Pretty bad                         Bad  \n\n[12167 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentences_count</th>\n      <th>characters_count</th>\n      <th>spaces_count</th>\n      <th>count_words</th>\n      <th>duplicates_count</th>\n      <th>chars_excl_spaces_count</th>\n      <th>emoji_count</th>\n      <th>whole_numbers_count</th>\n      <th>alpha_numeric_count</th>\n      <th>...</th>\n      <th>noun_phase_count</th>\n      <th>sentiment_polarity_score</th>\n      <th>sentiment_polarity</th>\n      <th>sentiment_polarity_summarised</th>\n      <th>sentiment_subjectivity_score</th>\n      <th>sentiment_subjectivity</th>\n      <th>sentiment_subjectivity_summarised</th>\n      <th>spelling_quality_score</th>\n      <th>spelling_quality</th>\n      <th>spelling_quality_summarised</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Очень хороший магазин сотрудники приятный</td>\n      <td>1</td>\n      <td>41</td>\n      <td>4</td>\n      <td>5</td>\n      <td>0</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.000000</td>\n      <td>Very bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Самый обычный продуктовый магазин. Есть сыры, ...</td>\n      <td>2</td>\n      <td>67</td>\n      <td>8</td>\n      <td>9</td>\n      <td>1</td>\n      <td>59</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>10</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.230769</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Вс комфортно</td>\n      <td>1</td>\n      <td>12</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.000000</td>\n      <td>Very bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Маленький филиальчик, необходимое есть. Две ка...</td>\n      <td>2</td>\n      <td>74</td>\n      <td>8</td>\n      <td>9</td>\n      <td>2</td>\n      <td>66</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>9</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.357143</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Плохо относятся клиентам!!!!!</td>\n      <td>2</td>\n      <td>29</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>27</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.625000</td>\n      <td>Bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12162</th>\n      <td>Персонал вежливый . Большой ассортимент</td>\n      <td>2</td>\n      <td>39</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.200000</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>12163</th>\n      <td>Скидки сыры. Скидки алкоголь. Приемлимые цены ...</td>\n      <td>7</td>\n      <td>146</td>\n      <td>20</td>\n      <td>20</td>\n      <td>2</td>\n      <td>126</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>18</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.296296</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>12164</th>\n      <td>Рядом домом, неплохая пятерочка, персонал хоро...</td>\n      <td>2</td>\n      <td>98</td>\n      <td>11</td>\n      <td>12</td>\n      <td>1</td>\n      <td>87</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>12</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.333333</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>12165</th>\n      <td>Хороший магазин дома. Кассиры приветливые. Про...</td>\n      <td>3</td>\n      <td>58</td>\n      <td>6</td>\n      <td>7</td>\n      <td>1</td>\n      <td>52</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.222222</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>12166</th>\n      <td>Норм, побольше сотрудников доставку выходным, ...</td>\n      <td>2</td>\n      <td>76</td>\n      <td>8</td>\n      <td>10</td>\n      <td>2</td>\n      <td>68</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.307692</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n  </tbody>\n</table>\n<p>12167 rows × 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"profiled_text_dataframe.to_csv(\"profiled_text_data_test.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T17:47:39.063693Z","iopub.execute_input":"2023-11-11T17:47:39.064217Z","iopub.status.idle":"2023-11-11T17:47:39.325482Z","shell.execute_reply.started":"2023-11-11T17:47:39.064178Z","shell.execute_reply":"2023-11-11T17:47:39.324287Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}